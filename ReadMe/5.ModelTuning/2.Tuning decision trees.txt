Tuning decision trees
Selecting transcript lines in this section will navigate to timestamp in the video
In the last video, we looked at tuning linear regression, and we saw that there's a lever that we can use with ridge regression to tune that. However, a lot of these linear models don't have a lot of tuning. Now, one of the reasons why I like decision trees is because it makes it really clear that you can pull these levers and have a big impact on your model. Let's look at our dt_pipe and we'll pull off the decision tree from that. And I'm going to look at the documentation from that. Here's the documentation. And if you look at this, there are a bunch of parameters down here. In VS Code, we have to hit this scrollable element to see a little bit more of that. Scroll this up a little bit. So here's the documentation for a decision tree. In the documentation, like I said, for Scikit-Learn, is very good. It tends to describe what's going on very well. If you go to the website, you can get links to papers that describes these algorithms, tells you what version these were added in. So you can see that there's a criterion parameter. This criterion is used to determine how to split the data when we're creating the nodes of the tree. You can change this to change the behavior of the tree. You can also change the max_depth. You can change the min_samples_split. You can think, and if you don't know what that is, here's the definition. It is the minimum number of samples required to split an internal node. If you think about how a decision tree works, it is looking at every feature and determining and sweeping through values in that feature to determine what is going to give us the best score if we split at that point. It tries that for all the features and determines which feature and value combination do the best split, and then it recursively does that. Remember, we saw that if you let max_depth go to none, which is the default, it will just basically memorize the tree and overfit. We can set max_depth to a lower number to simplify the tree. Min_samples_split is a little bit more fine-grained than max_depth. It is saying we have an individual node, and we have to have some number of samples or rows at that node to be able to split it into a new node. And if you think about that, one of the nice features of Scikit-Learn is that it has a naming convention that will help you understand what these parameters are doing. If a parameter starts with max, if you raise that value, it will tend to complicate the model, leading it towards overfitting. We can see that with max_depth here. As max_depth goes up, the model will get more complicated, it will memorize the data and overfit. If max_depth goes down, the model will be simpler. Another convention that Scikit-Learn has is parameters starting with min. Parameters starting with min do the opposite. If you raise the value of a min parameter, it will tend to simplify your data. And if you think about min_samples_split. If you have samples in a node and you raise that, then there will have to be more samples in there to split it. If that number is small, like there can be one or two samples in it, then it can start to overfit. So that's how you can understand what these parameters are doing and what impact they have on the model. There are a bunch of parameters in here. I'm not going to go over all of these. But again, a lot of them start with min and max. Remember, if you raise a max parameter, it will tend to overfit. If you raise a min parameter, it will tend to simplify your model. I'm going to do the same thing that I did with my ridge regression model. I'm going to sweep through some parameters for, in this case, max_depth. In this case, the parameter is called dt__max_depth because I am passing in the pipe. If I were just to pass in a decision tree, it would be called max_depth. Let's run this. So I've captured the results. What this is doing is it's sweeping over those different values, and it is doing what's called cross-fold validation. So it is splitting this data that we're passing in into some number of folds and evaluating the model against the held out fold. Let's look at a plot here, and I think it will make a little bit more sense. So we are sweeping the values of max_depth from one to 19. And we see two lines here. One is an orange line and one is a blue line. The orange line is the training score and the value that it's calling the cross-validation score, you can think of that as the test score. So for each value of max_depth, it is going to train a model multiple times using different portions of the data. And then it will look at the score with the training data and the score with the data that it hasn't seen, and it will track that. So we see that as we make max_depth increase, you can see that orange line is going up and up, the score is getting better. However, if we look at the score for the cross-validation or the testing data, you can see that as we increase max_depth, the score for the testing data improves to a point, and then it starts to drop off. When it starts to drop off, that is an indication that our model is overfitting. In this case, if I were to use this model, I would probably want to use this point here, which is probably around a max_depth of eight. Let's do that. I'm going to come in here and I'm going to make a pipeline with a max_depth of eight. I'm just going to set that max_depth parameter right there and look at our score. Our score for that is 0.75. We can also calculate the mean squared error and see how far off our predictions are with that model. Now we can contrast that with the default model. And you can see that our default model has an R2 score of 0.739 and the mean squared error is 198,000. So by using this tuned model, simply by changing the depth of that, our model has improved quite a bit. A takeaway that I want you to remember from this video is that convention of Scikit-Learn. Again, if you have a parameter that starts with max, if you raise that, that will tend to cause the model to be more complicated. If you lower that, it will tend to make the model simpler. And also for min parameters, if you raise that, it will tend to make the model simpler. A decision tree is a great way to illustrate overfitting and underfitting.



The “Tuning decision trees” video provides valuable insights into optimizing decision tree models, a crucial skill for your transition into machine learning. Here are the key takeaways:

Hyperparameters in Decision Trees: The video emphasizes the importance of hyperparameters such as max_depth and min_samples_split in controlling the complexity of decision tree models. Adjusting these parameters can significantly impact the model's performance by affecting how it generalizes to new data.

Balancing Model Complexity: It illustrates how increasing max_depth can lead to overfitting, where the model performs well on training data but poorly on unseen data. Conversely, increasing min_samples_split simplifies the model, potentially reducing overfitting but risking underfitting if set too high.

Practical Tuning Strategy: The instructor demonstrates a practical approach to tuning by sweeping through values of max_depth and observing the effects on model performance through cross-validation. This method helps in identifying the optimal balance between model complexity and generalization ability.


Understanding these concepts will be instrumental as you delve deeper into machine learning, providing a foundation for effectively tuning models to achieve the best possible performance.
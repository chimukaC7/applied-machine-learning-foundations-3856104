Tuning CatBoost
Selecting transcript lines in this section will navigate to timestamp in the video
In this video, we're going to tune our CatBoost model. CatBoost has a bunch of parameters. I'm not going to go over them, but at a high level, these parameters are impacting different aspects of the model. Some of these parameters impact what's called boosting, and you can think of boosting as each subsequent tree. So you can control how many trees there are, you can control the learning rate of the trees. You can think of, if we're going with that golf metaphor, you can think of that learning rate as how hard you're going to hit the ball. Are you going to hit it as hard as you can, or are you going to not hit it quite as hard? And oftentimes, if you hit it as hard as you can, you might hit it off to the side or you might go over. If you just focus on hitting it nice and easy, you might actually do better. And models when you lower the learning rate, often will do better. We also have parameters that deal with the tree. So how deep the tree is, how we grow the tree, similar to the decision tree. We have parameters that deal with sampling. And you can think of sampling as how many columns or features we want to look at. And you can change your trees to look at different samples at each level. And that helps to simplify your model. There is also a notion of regularization. And again, regularization is a way to tell it not to pay as much attention to certain columns. And also CatBoost has a cool feature that are called constraints. So we haven't really explored these, but you can have the notion of a monotonic constraint where you know that as some value goes up, the prediction should go up or down, and it shouldn't have a U-shaped or odd-shaped scenario, it should have a monotonic. As the value goes up, the prediction goes up, or as the value goes down, the prediction goes down. This is a really cool way to simplify your model, and I've seen a lot of my clients use this to make their models more understandable. Again, we can use Jupyter here and pull up the documentation. Just put a question mark after that and you'll get all the parameters. You'll note that some of these use the max and min convention, but not all of them do. Okay. I'm going to make a CatBoost regressor now and I'm going to say do 3,000 rounds. So you can hit the ball 3,000 times. I'm going to lower the learning rate. And I'm going to say early_stopping_rounds is 10. What early_stopping_rounds does is if the model hasn't improved in 10 rounds, then stop. Stop hitting the ball basically. Let's run this. And you can see that in this case, it stopped at round 357. So even though in the instantiation we said do 3,000 trees, if you look down here below, it stopped when it had 357 trees. So it didn't need to make all of those 3,000, it just needed to make a subset of those. We can also use a validation curve to understand what's going on here. Here, I'm going to check the depth of my trees. And so I'm going to sweep through values of the depth of my tree and see what happens to my model as I do that. Let's run this. This is taking a little bit of time. Remember, for each of those parameters, it's going to make a model. One thing to be aware of is that if you are using early stopping rounds with CatBoost or XGBoost, you want to provide this thing called an eval_set. You can think of the eval_set as data that it hasn't seen, that it can evaluate whether the model is improving or not. Okay. This is finished running. At this point, we should be able to make a plot here. Let's do that. And here is our validation curve with CatBoost. Again, we see two lines, an orange line and a purplish line. The orange line represents the training score, and the blue line is the score of the testing data. Let's just talk about what we're seeing here. We see that the score for the orange line improves as the depth goes up. And again, what that's telling us is that this is basically starting to memorize the data, and it's doing a good job of doing that. However, we really don't care about the orange line, we care about that blue line. So what am I seeing with the blue line? I'm seeing that as the depth goes up, it improves to a point and then it starts to level out, and then it actually starts to go down. And we can see that there is a shaded area which is the spread, how far it's doing. And it looks like when we have max_depth of nine, there's a large spread in there. So there are some segments of our data where it's able to do good predictions, but there are other segments where it does very bad predictions because the model is overfit. So what would I do with this model? One of the things that might be a little bit confusing here is that if you go from four, it looks like it goes down a little bit to five, but then it looks like it goes back up a little bit to six. So what would I choose there? My bias is when I have two options that are similar, I tend to choose the simpler option. So I would probably choose four here. The other thing I like about four is that the error bars are tighter around four than they are around six. Now we've just looked at one hyperparameter with CatBoost, but there are many hyperparameters with CatBoost, and we could probably improve this model even more by adding more trees or shrinking that learning rate to make it have better improvement, or even looking at some of the other hyperparameters as well. One thing to note is that max_depth is pretty blunt. If you think about this, we have, perhaps 300 trees in there. And we're saying that all of those trees have to be depth of four or depth of six. That's kind of a blunt instrument. We might want some of them to be slightly deeper, slightly shallower, or we might not want them to be even. And so we could use other hyperparameters to fine-tune that and probably make a better model. Okay. Here I'm going to set max_depth to four and look at our score. And our score with max_depth at four is 0.87. In this video, I showed you how you can tune one parameter of CatBoost, but we saw that CatBoost has a lot of parameters. So in the next video, we're going to talk about how we can tune more of those parameters.


The “Tuning CatBoost” video dives into the intricate process of fine-tuning a CatBoost model, which is essential for optimizing its performance. Here's a breakdown of the key concepts:

Boosting: Think of boosting as a team of golfers (trees) where each golfer learns from the previous one's mistakes to improve their shot. In CatBoost, this translates to each new tree model improving upon the previous ones, aiming to reduce errors.

Learning Rate: Using the golf analogy, the learning rate determines how hard you hit the ball. A high learning rate might make the model learn too fast, leading to poor predictions (like hitting the ball too hard and missing the target). A lower learning rate allows the model to learn more gradually, often leading to better performance.

Early Stopping Rounds: This is like giving the golfer a set number of attempts to hit the ball. If they don't make any progress in improving their shot within these attempts, they stop trying. In modeling terms, if the model's performance doesn't improve after a specified number of trees, the training stops. This prevents wasting time on a model that's not improving.

Max Depth of Trees: Imagine deciding how many clubs a golfer can use. A deeper tree (using more clubs) might make the model too complex, leading to overfitting, where the model memorizes the training data but performs poorly on new data. A shallower tree might not capture enough details. Finding the right depth is about balancing detail and generalization.

Regularization: This is a technique to simplify the model further, ensuring it doesn't pay too much attention to less important data features, similar to a golfer ignoring distractions and focusing on their swing.

Parameters Dealing with Sampling and Tree Depth: These parameters help decide how many features (columns) to consider at each split in the tree and how deep the tree should be. It's like choosing which golf clubs to use and how to approach each shot for the best outcome.


In summary, tuning a CatBoost model involves adjusting these parameters to find the right balance between learning too fast and too slow, making the model complex enough to learn effectively but not so complex that it can't generalize to new data. It's a delicate balance, akin to finding the right way to play a challenging golf course.
Hyperparameters and linear regression
Selecting transcript lines in this section will navigate to timestamp in the video
We've talked about how making a model is not really hard. It's like three lines of code. Prepping the data for the model involves more work, but looking at the output of the model and understanding how the model is behaving can take a little bit more time. And one of the things that we want to do with a model is to tune it. We've talked a little bit about this notion of overfitting, and there's also underfitting. So overfitting means that your model is too complicated that it's essentially memorizing the data. Underfitting means that your model is too simple and it's not able to capture the signal that's in your data. Ideally, we want a balance between those two. How do we do that? Hyperparameters are one way to do that. You can think of these as levers that allow us to control the behavior of the model. And at a high level, these levers allow us to control whether the model is more complex, leading us to overfitting or more simple, leading us to underfitting. Let's look at some examples here of using hyperparameters. Let's talk first about linear regression. Now we have our pipe here. And we can see our whole pipeline, what we're doing with all the columns. And then at the bottom, we have linear regression. And we can pull off from our pipe the linear regression model if we want to. We can use this named_steps attribute. And we can use this named_steps attribute and index into that the name that we put in the pipe to pull off the individual part of that. Here's the linear regression module from our pipe. Now, if you don't know what the parameters are for a part of your pipe, you can use the documentation to pull those out. I can just come in here and say, give me the help on that. And it will pull up the documentation. It turns out that the documentation in Scikit-Learn is actually pretty good. You can go through here and look at the parameters. There's parameters like fit_intercept, copy_X, n_jobs. It turns out that linear regression, as is in Scikit-Learn, really doesn't have parameters. It just is a model that when you run it, it's going to try and find the intercept and the coefficients. And there aren't really levers to tune that. However, there are other linear models that do offer that behavior. One of those is called ridge regression. So again, I'll use Jupyter here to import that and then I'll put the question mark here to pull up the documentation. And you can see that there is a parameter here called alpha. We can come down to the documentation and look and see what that is. This says that the documentation is truncated. So I can say click this to view it as a scrollable element. And we can go in here and find where alpha is. So as the alpha defaults to one and it is a constant, it multiplies the L2 term controlling regularization strength. It should be a non-negative float between zero and infinity. The documentation here is actually really good. It says when alpha is zero, the objective is equivalent to ordinary least squares. Essentially, it is linear regression, but as we bump up that number, it is regularizing and basically making it pay less attention to some of the columns or features in our data. So it turns out that we can't really make regression overfit more, but we can simplify it. This model doesn't really have a way to complicate it more, but it does have a way to simplify it. Let's try out this ridge regression. I'm going to make a pipeline for ridge regression. Let's run that. And here's the default value for that. It got a score of 0.8. Let's compare that with our linear regression model. And they have a similar score. Now note that ridge regression has an alpha value of one. Maybe we want to try different values and see how it compares. What I'm going to do is I'm going to sweep through a bunch of different values, and then I'll just keep track of my scores as I'm doing that. Let's run this code here. And at this point, we can plot the results of this. So here's our results. Looks like if we set that value to zero, our score is lower. And then if we set it up to 0.01, it goes up a little bit and we can track along here. And it looks like it's staying pretty consistent there at that value. In this case, it looks like there's a slight improvement. Again, one thing to be aware of in this plot is the Y-axis is going from 0.802 to 0.807. Looks like a very slight improvement. I would probably dive into this a little bit more, and maybe try out some different splits of the data to make sure that we're getting consistent improvement by doing this.


What is the purpose of using a grid search in hyperparameter tuning?
-Grid search is a hyperparameter tuning technique that systematically works through multiple combinations of hyperparameter values, conducting cross-validation for each to determine which combination gives the best performance.


What role do hyperparameters play in linear regression models?
-Hyperparameters are set before training and influence model behavior, unlike coefficients, which are learned from the data during training.
-In linear regression models, hyperparameters such as the regularization strength (in Ridge or Lasso regression) influence the model's behavior by controlling aspects like the complexity of the model, but they are not learned through training.


How can decision trees be tuned to improve model performance?
-Tuning decision trees involves adjusting hyperparameters such as the maximum depth of the tree and the minimum number of samples required to split an internal node to control the complexity of the model and prevent overfitting.

What are key hyperparameters to tune in CatBoost for optimal performance?
-In CatBoost, important hyperparameters include the learning rate (which affects how quickly a model learns), the depth of the trees (controlling the complexity of the model), and the number of iterations (how many trees to build). Tuning these can significantly impact model performance.
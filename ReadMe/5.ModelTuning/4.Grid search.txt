Grid search
Selecting transcript lines in this section will navigate to timestamp in the video
Up to this point, we've been looking at tuning a single hyperparameter, but a lot of these models have multiple hyperparameters. So how do we tune those? I'm going to show you one way to do that in this video, which is called grid search. You can think of grid search as brute forcing all of the different options for all of the parameters, and then tracking the scores for those and seeing which combinations do the best. So I'm going to start off here with our decision tree. Let's just train our basic out-of-the-box model. There it is. It has a score of 0.73. Again, we can look at this decision tree if we want to. Now what I'm going to do is I'm going to define some parameters that I want to train. And I'm going to look at max_depth, min_samples_split, and min_samples_leaf. So remember, max_depth is how deep the tree can go, min_samples_split is how many rows of data must there be in a node enabled to split that. Because that is a min, if you increase that, it makes it simpler. So if you lower that, if you say there can be two samples to split it, essentially you are memorizing the values below that. Similarly, we have min_samples _leaf. We have leaf nodes which are at the very bottom, how many samples can we have at a leaf? As that value goes up, we have a simpler model, as that value goes down, we are tending towards a more complicated model. Again, because I'm using my pipeline here, I'm going to prefix these with dt__, the dt coming from the name that I said in my pipeline up above here. So when we added this dt to the pipeline, I gave it the string dt, and that's how I refer to something in the pipeline to set a parameter there. Scikit-Learn has a tool called GridSearchCV. And we're going to make an instance of this. And then look at this, we just call fit on this. I've mentioned this multiple times but this is really cool with Scikit-Learn, the API is very simple. I'm going to kick this off because it's going to take a little bit of time to run. What this is going to do is it's going to loop through each combination of depth samples split and samples leaf and keep track of the scores and try and find the combination that does the best. Okay. This is finished now. Note that we just passed in the training data. So do I need the testing data? It's actually splitting up the training data into different cross-validations. That's what the CV means up here. And you can see that it's doing fivefold cross-validation. So it's taking the first 20 percent of that data and holding it out, training the model on the remaining 80 percent and then evaluating it on that 20 percent. And then, you can think of it as sliding the 20 percent to the next 20 percent, holding that out, making the model again and evaluating that. And then after it's done that for each of the five folds, it's going to give you an average and a standard deviation for that. After we've let that run, and this can run a long time. Another thing to note is that every time you add a new parameter in here, it doubles the time for this to run. So this took 16 seconds on my machine, but if we were to add one more parameter, this would probably double the time it takes to run. Once we've finished running that, we can inspect this attribute called best_params_. Remember that parameters that end with underscore are learned from fitting. So from fitting this, it learned that for this model, the best depth was nine, the min_samples_leaf was 10 and the min_samples_split was 10. So this might seem a little bit deeper, but you can see that these other parameters might be constraining it so that it's not always going towards nine. Let's make a decision tree now with those parameters. Here, I've got my pipeline. This pipeline looks the same. I could specify the parameters directly when I'm creating the decision tree regressor up here. Alternatively, I can say set params and I can take that dictionary and just put a star start in front of that. We call that unpacking and that will update the parameters for that. Let's run that and look at what our score is. Our score is 0.79 for that. And we'll compare that with our default model which is 0.74. In this case, you saw that we got a pretty big improvement by tuning those parameters. We only looked at a small subset of parameters and a small subset of values for those parameters. So it's entirely possible that there is some other combination that would make the model perform better. One of the great things about the grid search is that it's pretty simple to use. One of the bad things about it is that it takes a long time, and we have to manually input every parameter that we want to check. So I don't include it in this video, but for those who are motivated, you might want to check out some other libraries that do smarter optimization of parameters. One of the ones I like to use is called Hyperopt, which does Bayesian hyper-optimization of these parameters. In order to use this library, rather than specifying each value of the parameter, you specify a distribution, and then the library will pull values from the distribution, and when it finds values that do good, it exploits those values, searches that area around that. And every now and then, it will do an exploration step to make sure it doesn't get caught in a local minimum. That's generally what I like to use instead of grid search, because grid search can be slow.


The “Grid search” video introduces a critical concept in machine learning: tuning multiple hyperparameters to optimize model performance. Here's a simplified breakdown:

What is Grid Search? Imagine you're trying to find the best settings on a complex machine with several knobs (hyperparameters). Each knob can be set to different positions (values). Grid search systematically tests every possible combination of knob positions to find the one that gives you the best outcome. In machine learning, these “knob positions” are the hyperparameters like max_depth, min_samples_split, and min_samples_leaf for a decision tree model.

How Does it Work? Grid search creates a “grid” of all possible combinations of the hyperparameters you want to tune. It then trains a model for each combination and evaluates its performance. The combination that results in the best performance is considered the optimal set of hyperparameters.

Example: If you're tuning a decision tree and you want to explore 3 depths (e.g., 2, 4, 6) and 2 minimum sample splits (e.g., 2, 4), grid search will test all 6 combinations (2 depths x 3 splits) to find the best one.

Considerations: While grid search is thorough, it's also computationally expensive. Every additional hyperparameter exponentially increases the number of combinations to test. This means more time and resources are needed to complete the search.

Alternatives: Due to its exhaustive nature, grid search might not always be practical. Alternatives like “Hyperopt” use smarter strategies (e.g., Bayesian optimization) to search more efficiently. Instead of trying every combination, these methods focus on areas more likely to yield better results, saving time and computational resources.


In essence, grid search is a powerful tool for model optimization, offering a systematic way to explore the best hyperparameter settings. However, its computational demands mean it's important to use it judinally and consider alternatives when appropriate.
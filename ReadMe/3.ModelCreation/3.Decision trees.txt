Decision trees
Selecting transcript lines in this section will navigate to timestamp in the video
Okay. Let's look at another very popular model, which is a decision tree. A decision tree is a model that looks at each feature independently and determines what is the best value to split that feature to give us the best predictions. One of the cool things about a decision tree is that because it's not throwing data into n-dimensional space, we actually don't need to standardize our data to use a decision tree. And also because decision trees can look at features independently, some decision trees can even deal with categorical features or deal with missing data. That makes them convenient, in that, you can get away with less pre-processing of your data to use a decision tree. However, in our case, we've already done that pre-processing, so we don't need to worry about that. And it turns out that, for example, standardizing the data before throwing it into a decision tree is not going to change the results of that. And the rest of this looks pretty much the same as what we saw before, we're just swapping out linear regression with a decision tree regressor. Let's look and see what our score is from that. We'll run this code and our score is 0.73. What is that telling us? Remember our score previously for linear regression was 0.8 and this is 0.73. So this model is not performing as well as a linear regression model. That might seem weird because in the last section I said that this is a more complicated model. What's going on here? You might ask yourself why this is performing worse. What's going on is this is overfitting the data. A decision tree out of the box in Scikit-Learn will overfit the data. It will grow as deep as it needs to to have a single row in every node of this tree. So it's going to track every feature and find out what separates every row from every other one, and have an independent path through this tree. That's kind of cool in that it explains the tree, but what's not cool is that it tends to pick up the noise instead of the signal. So we can simplify our tree by pruning it and actually get a better score. We can actually prune it too much. And I'm going to say let's prune it to a depth of one. We actually have a name for this. It's called a decision stump. And this only makes one decision. And you can see that the score for our decision stump goes to 0.3. So there's probably some value between one and unbounded, where our tree will give us a better score than 0.73. And I've played around with this a little bit. You can just tweak this max_depth value and see what happens to your score. If we bump it up to nine, you can see that our score goes up a little bit above the default value. So somewhat counterintuitively, if you simplify the tree, it actually does better against data that it hasn't seen before. Again, that's because the tree is able to not focus on the noise that sets each individual row apart from each other row, but it focuses more on the signal that represents the data. If you're using decision trees, you'll want to remember that in Scikit-Learn, a decision tree out of the box will overfit, and you'll generally want to control how deep it goes.



The “Decision trees” video covers several important concepts about this popular machine learning model:

Nature of Decision Trees: They work by looking at each feature independently to determine the best value to split that feature for optimal predictions. This model's advantage is its ability to handle categorical data and missing values, making it versatile with less need for data preprocessing.

Handling Overfitting: A key challenge with decision trees is their tendency to overfit the data, meaning they can become too complex, capturing noise rather than the underlying pattern. The video explains how pruning (limiting the depth of the tree) can help address overfitting, making the model more generalizable to new, unseen data.

Practical Application and Tuning: The video demonstrates adjusting the max_depth parameter to find a balance between underfitting and overfitting. This tuning process is crucial for optimizing the model's performance on unseen data, highlighting the iterative nature of model development in machine learning.


Understanding these aspects of decision trees will be invaluable in your journey towards becoming a Machine Learning Engineer, especially as you delve into creating and tuning models for real-world applications.



Dummy model
Selecting transcript lines in this section will navigate to timestamp in the video
Okay. We are ready to start making a model. Remember, we're trying to predict the price from the other features of our data. Generally, when I'm making a model, I want to start off with something simple. Kind of for two reasons. One, if I can get something simple that works, that's awesome. But also if I have something simple, I can compare something more complex against that and see if the more complex model is giving me better answers. If the more complex model is not giving you better answers, then it might be an indication that my data doesn't have enough signal in it or I need to do some further feature engineering or processing of my columns to be able to pull out information from that. It turns out that Scikit-Learn has a very dumb model, quite literally. It's called the dummy regressor, and you can pull up the documentation for that if you need to. I'm going to use the strategy mean, which means that the dummy regressor will just predict the mean value. In this case, we really don't need machine learning to predict the mean value. We can just take all the prices, get the mean value, and say that that's the prediction. But because we are using Scikit-Learn, we have to create this pipeline to do this. And the cool thing about that is once we've created the pipeline to do it, we can swap out the different algorithms and try them out. And it's really easy to do that. So let's look at the code that I'm using here. I'm importing dummy regressor then I'm making an instance of dummy regressor, saying that the strategy we're going to use is mean. And then I'm going to create y. Remember y is what we're trying to predict. Because we're using polars, I'm going to make a DataFrame with one column in there. And then I'm going to use Scikit-Learn to split up my data into train_test_split. So this will give me a variable called x_train, x_test, y_train, and y_test. And then I'm going to throw that into my pipeline. Look how I do this. I've created my pipeline. So I've got a few steps. I've got the tweak step which is going to clean up my raw data. I've got the zip average price adder, which is going to add a new column that is the average price based on the zip code. I've got the preprocessor. We talked about that, that that looks at numeric columns and categorical columns and gets them ready for machine learning. And then at the end, I'm going to tack on my dummy regressor. And to use this pipeline now, I just call fit on that. And that will fit the model. I'm going to pass in my training data. I'm going to pass in both the x_train and the y_train. So again, x_train is the rows that represent every house with the features that describe that. And then y_train is the pricing information for each row. That will create my model and fit it. It's going to go through every step of the pipeline, and then at the end, I can call score. What score is going to do is it is going to give me what's called the coefficient of determination or the R2 score. This is a metric that allows me to evaluate my model, and it tells me, of the predictions that I gave, how much of the variance of those predictions is explained by the features or the columns in the data. This is a value that typically ranges between zero and one. If the value is zero, and it can go negative with Scikit-Learn, typically, doesn't go to negative, but it stays around zero and it goes up to one. If the features are close to one, that means 100 percent of the variance is explained by the columns. That means that it could be a good model. Let's run this and see what happens when we do this. Can we get a value there that's actually very close to negative? It's slightly zero. So what is this telling me? It's telling me that this is not a great model. That predicting the mean value, what is this telling me? It's telling me that our model that predicts the mean value does not do a good job of extracting information from the features in there. And if you think about that, that kind of makes sense. It's not really doing anything with the features in there. Again, because we are using Scikit-Learn, we can just inspect this dummy pipe here and see what's going on with each feature. In here, you can expand these to look at the parameters that describe them. And we can also make predictions. Now with this pipeline, I can say predict. I can pass in my test data. What this returns is an array. This is a NumPy array that represents the prediction for every row. In this case, it's the average value, which is apparently $537,000. In future videos, we'll look at models that are able to extract more information and do better predictions.


The “Dummy model” video introduces an important concept in machine learning: starting with a simple model. Here's a breakdown to help you understand better:

Why start with a simple model? Imagine you're learning to cook. You wouldn't start with a complex dish; you'd begin with something simple to understand the basics. Similarly, in machine learning, starting with a simple model helps us establish a baseline. It's like having a reference point to compare more complex models against later. If a complex model doesn't perform significantly better than your simple model, it might indicate that your data doesn't have enough useful information (signal) or that you need to refine your features (feature engineering).

What is a dummy regressor? Think of the dummy regressor as the most basic recipe in cooking. It doesn't try to understand the nuances of your ingredients (data); it simply predicts based on a simple rule, like predicting the average price of houses regardless of their features. This is useful because it sets a baseline performance - how well can we do without really trying? It's not meant to be a good model but a starting point.

How does it fit into the workflow? Using the dummy regressor is like sketching a rough outline before painting. You're setting up a structure (pipeline) that you can later fill in with more sophisticated algorithms. The pipeline includes steps like data preprocessing and feature engineering. Once you have this structure, swapping the dummy regressor with more complex models is straightforward, allowing for efficient experimentation.

Practical application: For you, transitioning to a Machine Learning Engineer role, understanding how to establish and improve upon a baseline model is crucial. It's not just about using the most complex algorithms but knowing when and how to apply them effectively. This approach ensures you're always making informed decisions and moving towards models that genuinely add value based on your data.


Remember, the journey in machine learning is iterative. Starting simple helps you understand the impact of each change and improvement you make along the way.



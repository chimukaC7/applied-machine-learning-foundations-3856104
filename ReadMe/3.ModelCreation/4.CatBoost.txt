CatBoost
Selecting transcript lines in this section will navigate to timestamp in the video
In this video, we're going to see how we can improve on our decision tree. Our dummy model is very simple. Linear regression is a little bit more complicated. Decision tree is more complicated than linear regression. One of the cool things about a decision tree is that it can look at a column and make a split based on that column, and then it can come back and look at the column again, which can help it model non-linear relationships between your data and what you're trying to predict. However, we also saw that a decision tree can overfit and in fact will overfit if you use the out-of-the-box settings for it. One of the ways we can get around that is using a model like CatBoost. CatBoost implements what is called a boosted algorithm, and I like to compare this to golf. If we say a decision tree is where I can hit the ball and it's going, the ball will be some distance after it lands away from the hole we're trying to hit it into. With boosting, we can hit the ball once and then we have subsequent hits where we can correct, we can make a new tree to try and hit the ball in from where it is to fix that error. And if you have that mental model of boosting, that's a good mental model, that golf mental model where a decision tree is one hit and boosting is hitting it multiple times. So CatBoost is not part of Scikit-Learn, but it implements the Scikit-Learn interface. One of the cool things about CatBoost is that it has native categorical support. One of the sad things about CatBoost is because polars is a new library, it does not support polars natively right now, so I'm going to have to tweak my pipeline a little bit. I'm actually going to make this pandas_transformer, which just converts my polars DataFrame into pandas. So we're going to stick that into our pipeline right here and then we're going to stick in cat at the end. Now the other thing we need to realize is that this two pandas is going to work on x, it won't work on y. So I'm going to convert my y to NumPy. So this is the code that will do that for fit and score. Okay. We see a little bit more output here. VS Code asks us if we want to install TensorBoard. I'm going to say, no, don't do that. Let's look at the output down here. We can see a lot more output out here. And what's happening is it's actually creating a thousand trees to hit that ball in the hole. But if you look at the score at the bottom, it is now 0.9. So this is a model that is a lot more powerful. It is more complicated, but it also has a coefficient of determination of 0.9, indicating that this model is performing better than both our basic decision tree and our linear regression model. You might ask yourself which model is the appropriate model to choose? And the answer is probably a somewhat unsatisfying answer, it depends. There are businesses that will choose to use a model like linear regression, even when it performs worse than something like CatBoost. Reason being is that linear regression is an explainable model. People would call that a white box model. We can explain what's going on there. CatBoost is a very powerful model, but it's hard to explain. In this case, it's making a thousand trees, and to be able to explain what's going on with each of those trees might be a little bit complicated. And if you're giving someone a loan and you're going to tell someone that they don't qualify for a loan, but if they raise their credit score by some amount, they will qualify, a logistic regression model will tell you that. However, CatBoost, with a thousand trees in it might be a little bit more complicated to explain. And so a business might choose not to use a model like CatBoost because they want to focus more on explainability than model performance.



Certainly! The “CatBoost” video introduces CatBoost as an advanced machine learning model that builds on the concept of decision trees to address some of their limitations, particularly overfitting. Here's a breakdown to help you understand the key concepts better:

Boosting and CatBoost: Boosting is a technique used to create a strong predictive model by combining multiple weak models, typically decision trees. Imagine you're trying to solve a complex puzzle. Instead of trying to solve it all at once, you tackle it piece by piece, each time improving on the areas where you were previously incorrect. That's similar to how boosting works. CatBoost, short for Categorical Boosting, is a specific implementation of this technique that excels at handling categorical data directly, without the need for extensive preprocessing.

Golf Analogy for Boosting: The video uses a golf analogy to explain boosting. Think of each decision tree as an attempt to hit a golf ball closer to the hole. The first hit (or tree) might not get the ball into the hole, but it gets it closer. Subsequent hits (or trees) aim to correct the previous shot's mistakes, getting the ball even closer each time. In CatBoost, this process involves creating hundreds or even thousands of trees, each improving on the errors of the previous ones, to achieve a highly accurate model.

Native Categorical Support: One of the standout features of CatBoost is its ability to natively handle categorical data (like gender, country, etc.), which can be challenging for many other models. This means less time spent on data preprocessing and more focus on model development.

Model Complexity and Explainability: While CatBoost can provide high accuracy, it's also more complex than simpler models like linear regression. This complexity can make it harder to explain how the model arrived at its predictions, which might be a consideration depending on the application. For example, in situations where you need to explain decisions to customers or comply with regulations, a simpler, more explainable model might be preferred despite lower accuracy.

Practical Consideration: The video also touches on practical aspects of implementing CatBoost, such as the need to convert data formats for compatibility. This highlights the importance of understanding not just the theoretical aspects of machine learning models but also the practical considerations that come with their implementation.


Understanding these concepts is crucial as you pivot towards a Machine Learning Engineer role. It's not just about choosing the most powerful model but selecting the right tool for the job, considering factors like data type, model accuracy, and the need for explainability.


CatBoost can naturally handle categorical variables without requiring any prior transformation or explicit encoding, which simplifies the data preprocessing step and can lead to better model performance.
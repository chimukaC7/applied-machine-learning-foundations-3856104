Scikit-learn pipelines
Selecting transcript lines in this section will navigate to timestamp in the video
This video is going to talk about Scikit-Learn pipelines and Scikit-Learn pipelines are not something that are strictly required, but I think when you do things that are more complicated, they are going to allow you to make your code easier to use. So it's one of those things where if you're starting out, it looks like a lot of overhead, but once you get more complicated, they actually come to your rescue. I've got a lot of imports up here. Let me quickly talk about them. We've got a pipeline from Scikit-Learn. You can think of a pipeline as a sequence of steps that we're going to put our data through. We have a ColumnTransformer. A ColumnTransformer is a tool that allows us to run certain steps just on specific columns, not on everything in there. I've got the StandardScaler and the OneHotEncoder. A StandardScaler standardizes the data, meaning it gives each column a mean value of zero and a standard deviation of one. And a OneHotEncoder is a mechanism for taking categorical data, because most machine learning algorithms don't work with text data or categorical data, and it encodes that into numeric values. We have the SimpleImputer that fills in missing values. I've got the train_test_split function. This is going to allow us to split a data set into a training set and a testing set. The idea there is that we want to make sure that when we evaluate our model, we evaluate it on data that it hasn't seen so we get a feel of how the model will work when it comes across data that it hasn't seen before. FunctionTransformer is a class from Scikit-Learn that allows me to convert a function into a transformer to stick into a pipeline. And then I've got BaseEstimator and TransformerMixin. This is when we are defining our own Scikit-Learn transformer, we need to subclass these. At the bottom here I'm importing the Scikit-Learn set config method and Scikit-Learn as of recent versions outputs polars, which is kind of cool. Okay. Let's run that and make sure that that works. I'm going to select the cell, hold down control and hit "Enter" and make sure that that runs. It looks like we're good there. Okay. Let's look at what our numeric columns are. We want to treat our numeric columns differently from our categorical columns. There is our numeric columns. And what I would do is just print those out. I'm using this cs.numeric that's a polars selector to select just the columns that are numeric. I'm just printing those out. You can see that this prints out what looks like a Python list. And then I might copy some of those and put them into a list like this. So I'm going to use a subset of these. We'll use the number of bedrooms, the number of bathrooms and the square-foot living. And I'm going to standardize these. Again remember that notion when you're throwing things into high dimensions. If you were to compare square-foot living with number of bathrooms, those have different scales and certain algorithms will pay more attention to the square-foot living because it's so much larger than the number of bathrooms. Where in reality, if you're trying to make a pricing model of a house, both of those are important, and we want to consider both of those somewhat independently. Standardizing our data is a way for us to take out the variance as a feature and just let it look at the data that is in there. Let's see how this works with Scikit-Learn. We're going to make an instance of the scaler and then I call fit_transform. One of the cool things about Scikit-Learn is that it has a consistent interface. And once you understand this interface, it's going to be used all over the place. So fit_transform actually does two things. It fits the StandardScaler, trains it to know how to transform every column, to give it a mean value of zero and a standard deviation of one. And then transform actually does the transformation. So I'm going to, from my raw data, I'm going to call tweak_housing which cleans it up a little bit then I'm going to select the numeric features and throw that into this method. Let's run this and see what happens when we do that. And you can see that we get out a polars DataFrame, because up above we set the configuration to output polars. This is the output that we get. And you can see that bedrooms and bathrooms and square-foot living now have negative values in that. That's because this has standardized that output and every column now should have a mean value of zero and a standard deviation of one. Okay. So in the previous example, I showed you just how to use the StandardScaler, now I'm going to show you how to stick that into a pipeline. A pipeline remember can have multiple steps in it. This one is just going to have one step. We're just going to give a tuple that has the name and the scaler or the transformer that we want to stick into it. And then look at this. We're going to call pipeline.fit_transform. It is the same method that we're using up above. Remember Scikit-Learn reuses the same methods all over the place. Let's run that and we get the same output as we did above. So now I'm going to add another step to my pipeline. I'm going to add an imputer as well. And this is using the SimpleImputer. It's just saying if a value is missing add the median value. And you can see me running this. When I run this, I'm going to call fit_transform and now I shouldn't have any missing values in there. Let's look at how to deal with categorical features. In this case, I only have one. It's the zip code. I'm going to use the OneHotEncoder class, and I'm going to tell it if there are unknown categories, to ignore those. What that means is, in the future, if we have trained it on a data set that has certain categories, and then when we try and do prediction, if it comes across a new category, it won't raise an error, it will just ignore those. And if you look at this, the code is exactly the same, we call fit_transform. In this case, I'm just doing this on the categorical features. However, we got an error here. Let's look and see what's going on. When we run this, I'm going to scroll down to the bottom and it says polars output does not support sparse data. So we actually need to say sparse output is false here if we want to run this with polars and we get something that looks like this. Now if you look at the shape of this, you'll see that there are 70 columns here. You might not want 70 columns, so we might want to prune that back a little bit. And we can do something like this where we say max_categories is 10. Again, let's just run that to make sure that that works. And it looks like we got 10 categories or columns there. Again, I can stick this into a pipeline. This pipeline only has one step. I'm showing in this example that I can also set parameters on the pipeline. In this code, you see I call the set_params method after creating the pipeline, and I can override or set a parameter for one of the steps in the pipeline. So you take the name, in this case, the name is cat. You put two underscores after that, and then you give it the parameter that you want. And here I'm going to say that the max categories is 10. Let's just run that and make sure that that works. And it does. Let's see how we can use a ColumnTransformer. Remember, ColumnTransformer lets us apply specific transformations to certain columns. Generally, we're going to want to apply numeric transformations to number columns and categorical transformations to categorical columns. In this case, I've got a numeric_transformer, which is a pipeline for imputing and then standardizing the numbers. And then I'm just going to stick my OneHotEncoder in here. And look at this, we just called fit_transform. So it's that same method there. And we get an output that looks like this. Again, I've said this a few times, but the nice thing about Scikit-Learn is that it's consistent. Okay. Let's look at how to make a custom transformer. Here, I'm defining my own transformer. I'm using a Python class. You just need to subclass BaseEstimator and TransformerMixin and then you need to implement a fit and a transform method. What is this transformer going to do? It's going to map a zip code to the average price of that zip code. So in the fit method, we will take in an X as input, that's a polars DataFrame. I'm going to group that by the zip code and I'm going to aggregate that to get the average price for each zip code. And then I'm just going to return self. That's the standard interface there. And then transform is where I transform data. And you can see that I'm going to get in a DataFrame here and I'm going to add the zip price average information on that. Let's make an instance of that. And look, we can call fit_transform and see what it does when we do that. And you can see that this has added a new column called zip_mean. And then we're going to take all that and stick this into this pipeline at the bottom. So the first thing we're going to do is call tweak. That will happen on everything. Then we will add our average column, and then we will call our preprocessor which splits it out, as we saw up here, into numeric and categoricals. I'm going to make this variable called X, which is going to be all of my columns. And then I'm going to say y is the price and I'm going to split my data. So I'll have the training data and the testing data. And then I'm going to say let's call fit_transform on my raw data with my price and see what happens when we do that. And you can see that we're running this through that whole pipeline now. One thing to note is that unlike pandas, you would use a series for y impellers. If you try and stick in a polars series, which is this right here, doing an index operation instead of doing a select there, you will get an error here. So you need to, in polars, if you're passing in that y, you need to pass in a DataFrame with a single column in there. Okay. At this point, we have our pipeline ready. We're ready to make models.




The “Scikit-learn pipelines” video introduces a powerful concept in data science and machine learning that helps streamline the process of data preprocessing and model training. Here's a breakdown to help you understand better:

Pipelines in Scikit-Learn: Think of a pipeline as a conveyor belt in a factory where each section of the belt represents a step in processing your data or applying a machine learning model. Each step is automated and follows a sequence, ensuring that data flows smoothly from one process to another without manual intervention.

Why Use Pipelines?: Initially, pipelines might seem like extra work, especially for simple tasks. However, as your data processing becomes more complex, pipelines become invaluable. They help keep your code organized, make it easier to reproduce your work, and reduce the risk of errors, such as applying training parameters to a test dataset.

Components of a Pipeline:

Transformers: These are steps that prepare your data. They might standardize it (giving each feature a mean of zero and a standard deviation of one), fill in missing values, or convert categorical data into numeric data. Transformers end with a method that transforms the data.

Estimators: This is typically your machine learning model. An estimator predicts outcomes based on the data.

ColumnTransformer: This tool allows you to apply different transformations to different columns of your data. For example, you might want to standardize numeric data but one-hot encode categorical data. ColumnTransformer lets you specify which columns receive which treatment.

Practical Example: The instructor demonstrates how to standardize some numeric features (like the number of bedrooms) using StandardScaler, and how to encode a categorical feature (like zip code) using OneHotEncoder. This is crucial because machine learning models require numeric input, and features need to be on a similar scale to perform well.

Custom Transformers: You can create your own transformers for tasks specific to your dataset. For example, the video shows how to create a transformer that maps a zip code to the average price of that zip code. This is useful when you have domain-specific knowledge that can enhance your model.

Putting It All Together: The final part of the video shows how to combine all these steps into a single pipeline. This includes preprocessing (like standardizing and encoding), custom transformations, and even splitting the data into training and testing sets. The pipeline ensures that all these steps are applied consistently, making your model more reliable and easier to understand.


By using Scikit-Learn pipelines, you're not just making your code cleaner and more efficient; you're also ensuring that your data processing and model training are reproducible and error-free. This is especially relevant in your journey towards becoming a Machine Learning Engineer, as it allows you to focus on improving your models rather than worrying about data preprocessing steps.
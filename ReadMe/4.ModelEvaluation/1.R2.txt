R2
Selecting transcript lines in this section will navigate to timestamp in the video
In these next sections, we're going to talk about evaluating your model. Now just an aside, one thing to be aware of is that we are using GitHub Codespaces here, and it turns out that I just came back to recording this video and GitHub Codespaces kicked me out. It needed to be restarted. So I'm going to, so I'm going to show you what to do if that happens to you. For example, if I come down here right now and try and run this code, I get a name error. That's because when Codespaces kicked me out, I basically lost my code that was running. So what I'm going to want to do is I'm going to want to load my data again and to load my data, I'm going to have to run this code up here at the start. And then I'm going to have to create the pipeline code as well. So I'm going to come down here into the section where I created my tweak function. I'm going to want to have that. And then I want this pipeline code. So one thing that you might want to do is take all the cells that I'm running right now and move them up to the top or consolidate them. Then the next time you come back to your notebook, you can run just the code cells at the top without having to scroll through and find all the different cells to run. I think I can run this one here at the bottom. Let me just try and run this one and see if this will work here. Okay. So that one appeared to be working. And let's just go through some of our pipelines here from our previous one to see if we can run, for example, this CatBoost pipeline. Okay. So I think I'm pretty good here and I should be able to come back and run this code cell here. Awesome. Let's discuss model evaluation. As we saw, creating a model with Scikit-Learn is not many lines of code. In fact, you could make a model using a library like CatBoost or XGBoost without going through a bunch of pre-processing, because these tree-based models don't require a lot of pre-processing and just call fit on the model. And that would be like three lines of code to do that. So it's not particularly hard to make a model, sometimes these pipelines can be a little bit more complex. That might involve some work. But another thing is the work after making the model, evaluating your model, determining if it will work for your business, and if the model actually provides value. A common metric, and we've talked about this already, is this R2 or coefficient of determination score. And when you call that score with a regression model, it will spit this out for you. So we can see this right here. With our CatBoost model, it gets a 0.90 score on that. Again what this is telling us is that 90 percent of the variability is explained by the columns in there. That the output is explainable by the features, which is what you want. You want the features to be able to explain the output. Now let me just make a commentary on R2. A lot of times when I'm teaching or discussing R2, folks will ask me, my R2 score is, and you can pick some value, 0.5, 0.3, 0.9, 0.99 and they will say, is that score good enough? And my general answer is this, if you have an R2 score that's very low, like zero, that's typically not a good model. If you have an R2 score that's very high like one, typically what that tells me when the score is very high is that you probably don't need a model because it is able to make the predictions all correctly, or what it tells me is that you're cheating, in that, you have some other feature in your data that knows what the result is, and you can derive the result from that. If you already know what the result is, or you already have a feature that you can derive the result from that, you really don't need a model. So typically we don't see model scores of one, unless you're cheating or you don't need a model. Model scores of zero are not too uncommon, but that typically says that the model is not good. Now for all of the values in between there, whether a score is good or not is more of this, it depends. What we can typically say is that a model that has a higher R2 score has better performance than another model, but that in and of itself typically doesn't tell us whether a model would be suitable for business. Typically, you would want to attach a business metric to your model to see how much money you would save or how much money you would lose if you deployed the model. And that can give you further insight into whether you should use a model or not.


Certainly! The “R2” video is all about evaluating how good your machine learning model is at predicting outcomes. Let's break it down:

Imagine you're trying to guess your friends' heights based on their shoe sizes. You come up with a method (your model) to make these guesses. Now, you want to know how good your method is. That's where R2, or the coefficient of determination, comes into play.

Think of R2 as a scorecard for your guessing game. It ranges from 0 to 1, where:

A score of 0 means your guesses are no better than just always guessing the average height of all your friends, regardless of their shoe size.
A score of 1 means your guesses are spot on. You can predict their heights perfectly based on their shoe sizes.

So, if your method gets an R2 score of 0.90, it's like saying you can guess 90% of your friends' heights accurately based on their shoe sizes. The higher the score, the better your method is at making predictions.

However, the video also warns about two things:

Very Low Scores: If you get a score close to 0, it's like saying your method isn't really better than random guessing. Maybe shoe size isn't a good way to guess height, after all.
Very High Scores: If you get a perfect score (or close to 1), you might be cheating without realizing it. Maybe you're using something that directly tells you their height, in which case, you're not really “guessing.”

In summary, R2 helps you understand how well your model works. It's like a report card showing how good your method is at making predictions, with the goal being to get as close to 1 as possible, without cheating.



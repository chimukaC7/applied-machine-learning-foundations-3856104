Residual plot
Selecting transcript lines in this section will navigate to timestamp in the video
So we've talked about evaluating models using metrics. I want to talk about another mechanism for evaluating a model using a graphical representation. And that's a residuals plot. What we're going to do is we are going to take our model and we are going to calculate the residuals. The residuals are the error, how far off the predictions are. And you can take the true values and subtract the predictions and that will give you the residuals. So if your residual is high that means that your prediction was too low. If your residual was a positive number, that means that your prediction was too low. Conversely, if your residual is a negative number, that means that you're predicting too high. What we want to see in this plot is as we go through our predicted values, we want to see a balance of high and low. We don't want to see consistently high or consistently low. Ideally, we would like to see those errors spread out, both positive and negative. Another thing that we're going to look at is what's called the heteroscedasticity. That's a big word, but basically what it means is, as we go through our predictions, we want to make sure that we aren't getting a wider variance or a smaller variance, that we have consistent variance across our predictions. Let's look at a plot and maybe we can make some more sense of this. So I'm going to manually create a residuals plot using matplotlib here. And in the X, I have the predicted price, in the Y, I have the residuals. So let's talk about what I want to see here. Remember I said I want to see errors that are both positive and negative. And it looks like I'm seeing that. And then the other thing I want to see is as I sweep through that predicted price, I want to see consistent variance that we're not spreading out too much or seeing some funneling behavior. It's a little bit hard to see here. Looks like the vast majority of our predictions are in this ball over here on the left-hand side. As our prices do get larger, we do seem to see a little bit more error, which we would expect, but it's not fanning out a lot. So I think this is an okay plot. I'll note that we created this with matplotlib. There are libraries like Yellowbrick that will do this for you. However, I'm going to use polars to do this. And here's the code to do this with polars. Let me just walk through this code and because it's a chain, it makes it really easy to just comment this out. So here's our y_test. I'm going to make a new column called the predicted price and the residual. And then what I'm going to do is I'm going to scatter in the X-axis the predicted price and in the Y-axis, the residual. And I'm going to adjust the formatters there so it prints it as dollars. The nice thing about this plot is because it's using bokeh on the back end, I can hover over points and I can get their true values. It looks like we are seeing pretty consistent behavior. I'm not seeing a lot of spreading out there, and most of our data is here between the zero and one million range. One thing that you can do with a residuals plot is check whether you have nonlinearities in your data. Generally, a model like CatBoost will do okay with capturing those, but if you're using something like linear regression, you might see a residual plot that is curving, and that would be an indication that you would want to add a nonlinear column to help linear regression be able to make the prediction. Another thing that you can use this for is to detect overfitting. Let me show you how we're going to do that. I'm going to make a function that creates this residuals plot, again using polars, but I'm going to pass in the training data and the testing data and I'm going to plot both of those. So let's run that. Now note that in this case, I have added a new column called type. And then when I'm coming down here to plot, I just say by type. Cool thing with polars is that it will color both of those with a different color. So we can see that the blue is my testing data and the red is my training data. Ideally, what I want to see is similar behavior with both of these. If I see that the training data has very low residuals, but the testing data does not, that wouldn't be an indication that I'm overfitting. It doesn't look like I'm seeing too much of that here. I am seeing that blue spreads out a little bit more than red, which is probably to be expected with a gradient-boosted decision tree. We could probably do some tuning and improve the model a little bit. Let's contrast this with a decision tree. Here, I'm going to do this same plot here with a decision tree. And we can see clear evidence of overfitting here. You can see that that red training error is very low, indicating that our decision tree is overfitting. It's essentially memorizing the training data where the testing data has a high residual. Also, we are seeing a little bit more evidence of this funneling going out here. Another thing that you need to be careful with is the Y-axis in here. On this one, it looks like we're going from -4,000,000 to 3,000,000. In our CatBoost, we have a different scale up here. So just be aware of that. This does look like it's tighter, but also we are zooming out a little bit. Nice thing with this bokeh is we can come in here and we can zoom in to that and see what's going on there. One more convenient thing that you can do here is you can find where you have huge errors. So these points where we have pretty big residuals, we might want to go in and actually look at the data and see why we are making bad predictions. This plot makes it clear that there are certain points that our model does not perform well with. Contrast that with just a numeric evaluation like the R2 or the mean squared error. Those just give us an overall feel, but don't let us look at the individual points. I like to combine both metrics and visualizations to get a good feel of how my model is performing.



The “Residual plot” video you're watching delves into a crucial aspect of evaluating machine learning models, specifically focusing on visualizing the errors (or residuals) a model makes in its predictions. Let's break down the key concepts to make them easier to grasp:

What is a Residual?
Residuals are the differences between the actual values and the values predicted by your model. If you predicted a house to cost $300,000 but its actual price was $320,000, the residual would be $20,000.

Understanding Residual Plots
A residual plot is a graph that shows these residuals on the y-axis and the predicted values on the x-axis. It helps you visually assess how well your model is performing across different ranges of predictions.

Key Points from the Video:
Balance of Errors: You want your residuals to be scattered evenly above and below the zero line on the plot. This indicates that your model is just as likely to overestimate as it is to underestimate, which is a good sign of accuracy.

Consistent Variance (Heteroscedasticity): The spread of residuals should be consistent across all levels of predicted values. If the spread increases or decreases with the predicted value, it suggests that the model's performance varies at different scales, which is not ideal.

Using the Plot:

Detecting Nonlinearities: If the residuals form a pattern (e.g., they curve), it suggests your data has nonlinear relationships that your model isn't capturing.

Identifying Overfitting: By comparing residuals from training and testing data, you can see if your model performs much better on the training data, which might indicate overfitting.

Practical Application: The video shows how to create a residual plot using Python libraries like Matplotlib and Polars. This hands-on approach not only helps in understanding the concept but also in applying it to real-world datasets.


Simplified Example:
Imagine you're trying to guess the weight of backpacks based on their size, and you plot your guesses against the actual weights. If your plot shows guesses scattered evenly around the “perfect guess” line (actual weight), with no clear pattern and consistent spread, you're on the right track. If not, the plot can guide you on how to adjust your guessing strategy.

In summary, residual plots are powerful tools for diagnosing and improving machine learning models. They provide a visual means to identify issues like overfitting, underfitting, and nonlinearity, enabling you to refine your models for better accuracy.

